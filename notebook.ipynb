"metadata":{},"id":"d3d001b0-2e2f-4b58-8442-99520bad831f","cell_type":"markdown"},{"source":"# All required libraries are imported here for you.\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load the dataset\ncrops = pd.read_csv(\"soil_measures.csv\")\n\n# Display the first few rows of data to understand the structure\nprint(crops.head(5))\n\n# Encode the target variable if it is not already categorical\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(crops['crop'])\n\n# Separate features (X)\nX = crops.drop(columns=\"crop\")\n\n# Initialize variables to store the best feature and its score\nbest_score = 0\nbest_feature = None\n\n# Iterate through each feature to find the best predictive feature\nfor feature in X.columns:\n    X_feature = X[[feature]]\n    \n    # Split data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X_feature, y, test_size=0.3, random_state=42)\n    \n    # Train a logistic regression model\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    \n    # Predict on the test set\n    y_pred = model.predict(X_test)\n    \n    # Calculate accuracy score\n    score = accuracy_score(y_test, y_pred)\n    \n    # Check if this feature gives a better score\n    if score > best_score:\n        best_score = score\n        best_feature = feature\n\n# Create a dictionary with the best predictive feature and its score\nbest_predictive_feature = {best_feature: best_score}\n\n# Print the best predictive feature and its score\nprint(\"Best Predictive Feature:\", best_predictive_feature)\n","metadata":{"id":"bA5ajAmk7XH6","executionTime":3333,"lastSuccessfullyExecutedCode":"# All required libraries are imported here for you.\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load the dataset\ncrops = pd.read_csv(\"soil_measures.csv\")\n\n# Display the first few rows of data to understand the structure\nprint(crops.head(5))\n\n# Encode the target variable if it is not already categorical\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(crops['crop'])\n\n# Separate features (X)\nX = crops.drop(columns=\"crop\")\n\n# Initialize variables to store the best feature and its score\nbest_score = 0\nbest_feature = None\n\n# Iterate through each feature to find the best predictive feature\nfor feature in X.columns:\n    X_feature = X[[feature]]\n    \n    # Split data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X_feature, y, test_size=0.3, random_state=42)\n    \n    # Train a logistic regression model\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    \n    # Predict on the test set\n    y_pred = model.predict(X_test)\n    \n    # Calculate accuracy score\n    score = accuracy_score(y_test, y_pred)\n    \n    # Check if this feature gives a better score\n    if score > best_score:\n        best_score = score\n        best_feature = feature\n\n# Create a dictionary with the best predictive feature and its score\nbest_predictive_feature = {best_feature: best_score}\n\n# Print the best predictive feature and its score\nprint(\"Best Predictive Feature:\", best_predictive_feature)\n","executionCancelledAt":null,"lastExecutedAt":1721322112572,"lastScheduledRunId":null,"lastExecutedByKernel":"2839937a-4d24-445c-b484-3d898600569d","outputsMetadata":{"0":{"height":164,"type":"stream"}}},"id":"d0eb4f16-5a99-460d-a5ba-706b7ef0bbe7","cell_type":"code","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":"    N   P   K        ph  crop\n0  90  42  43  6.502985  rice\n1  85  58  41  7.038096  rice\n2  60  55  44  7.840207  rice\n3  74  35  40  6.980401  rice\n4  78  42  42  7.628473  rice\nBest Predictive Feature: {'K': 0.2803030303030303}\n"}]},{"source":"","metadata":{"executionCancelledAt":null,"executionTime":137,"lastExecutedAt":1721322112709,"lastExecutedByKernel":"2839937a-4d24-445c-b484-3d898600569d","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":""},"cell_type":"code","id":"05dda91a-9f25-416e-9e95-9a02e68c560c","outputs":[],"execution_count":2}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}
